{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36486aa4-ec5c-45b2-b0e3-b0b8a3b580fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from math import exp\n",
    "from hyperopt import fmin, hp, tpe, SparkTrials, STATUS_OK\n",
    "import tempfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importance(model, x):\n",
    "  feat_imp = pd.DataFrame(model.feature_importances_, index = x.columns).reset_index()\n",
    "  feat_imp.columns = ['feature', 'feature_importance']\n",
    "  feat_imp = feat_imp.sort_values(by = 'feature_importance', ascending = False)\n",
    "  feat_imp['cumulative_feature_importance'] = feat_imp['feature_importance'].cumsum()\n",
    "\n",
    "  f = 'gain'\n",
    "  feat_gain = pd.DataFrame.from_dict(model.get_booster().get_score(importance_type = f), orient = 'index').reset_index()\n",
    "  feat_gain.columns = ['feature', 'gain']\n",
    "\n",
    "  return feat_imp.merge(feat_gain, on = 'feature', how = 'left').sort_values('gain', ascending = False)\n",
    "\n",
    "def calculate_model_evaluation_metrics(y, y_hat, wt, bins = 20, mode = None):\n",
    "  report_tmp = pd.DataFrame(y)\n",
    "  report_tmp['probability'] = y_hat\n",
    "  report_tmp.columns = ['bad', 'probability']\n",
    "  report_tmp['weight'] = wt\n",
    "  report_tmp['good'] = 1 - report_tmp['bad']\n",
    "  if mode == 'score':\n",
    "      report_tmp = (report_tmp.sort_values(by = (['probability', 'bad']), ascending = [True, False])).reset_index(drop = True)\n",
    "  else :\n",
    "      report_tmp = (report_tmp.sort_values(by = (['probability', 'bad']),ascending = [False, False])).reset_index(drop = True)\n",
    "  report_tmp['weight_cumulative_sum'] = report_tmp['weight'].cumsum()\n",
    "  report_tmp['weight_good'] = report_tmp.apply(lambda x: 0 if x['bad'] == 1 else x['weight'], axis = 1)\n",
    "  report_tmp['weight_bad'] = report_tmp.apply(lambda x: 0 if x['good'] == 1 else x['weight'], axis = 1)\n",
    "  report_tmp['bucket'] = pd.cut(report_tmp.weight_cumulative_sum, bins)\n",
    "  report_grp = report_tmp.groupby('bucket', as_index = False)\n",
    "  report = report_grp.min().probability\n",
    "  report = pd.DataFrame(report_grp.min().probability, columns = ['decile'])\n",
    "  report['decile'] = pd.Series(list(range(10, 110, int(100/bins))))\n",
    "  report['minimum_probability']= report_grp.min().probability\n",
    "  report['maximum_probability'] = report_grp.max().probability\n",
    "  report['bads'] = report_grp.sum().weight_bad\n",
    "  report['goods'] = report_grp.sum().weight_good\n",
    "  report['total'] = report.bads + report.goods\n",
    "  report['bad_rate'] = (report.bads/report.total)\n",
    "  report['cumulative_bad_rate'] =(report.bads/report_tmp.weight_bad.sum()).cumsum()\n",
    "  report['cumulative_good_rate'] =(report.goods/report_tmp.weight_good.sum()).cumsum()\n",
    "  report['ks'] = np.abs(np.round(report['cumulative_bad_rate'] - report['cumulative_good_rate'], 4))\n",
    "  flag = lambda x: '<' if x == report.ks.max() else ''\n",
    "  report['max_ks'] = report.ks.apply(flag)\n",
    "  roc_auc = roc_auc_score(y, y_hat, sample_weight = wt)\n",
    "  gini = 2 * roc_auc - 1\n",
    "  report['roc_auc'] = roc_auc\n",
    "  report['gini'] = gini\n",
    "  report = report[[\"decile\",\n",
    "                   \"minimum_probability\",\n",
    "                   \"maximum_probability\",\n",
    "                   \"bads\",\n",
    "                   \"goods\",\n",
    "                   \"total\",\n",
    "                   \"bad_rate\",\n",
    "                   'cumulative_bad_rate',\n",
    "                   'cumulative_good_rate',\n",
    "                   'ks',\n",
    "                   'max_ks',\n",
    "                   'roc_auc',\n",
    "                   'gini']]\n",
    "  return report\n",
    "\n",
    "def model_evaluation_report(y_train, y_hat_train, y_val, y_hat_val, w_train, w_val, bins = 10 , mode = None):\n",
    "  train_report = calculate_model_evaluation_metrics(y_train, y_hat_train, w_train, bins, mode)\n",
    "  val_report = calculate_model_evaluation_metrics(y_val, y_hat_val, w_val, bins, mode)\n",
    "  summary = pd.DataFrame({}, index = list(range(0,5)), columns = ['train', 'validation'])\n",
    "  summary['train'][0] = train_report.roc_auc[0]\n",
    "  summary['train'][1] = train_report.gini[0]\n",
    "  summary['train'][2] = train_report.ks.max()\n",
    "  summary['train'][3] = train_report.query('decile == 10')['cumulative_bad_rate'].reset_index(drop = True)[0]\n",
    "  summary['train'][4] = train_report.query('decile == 20')['cumulative_bad_rate'].reset_index(drop = True)[0]\n",
    "  summary['validation'][0] = val_report.roc_auc[0]\n",
    "  summary['validation'][1] = val_report.gini[0]\n",
    "  summary['validation'][2] = val_report.ks.max()\n",
    "  summary['validation'][3] = val_report.query('decile == 10')['cumulative_bad_rate'].reset_index(drop = True)[0]\n",
    "  summary['validation'][4] = val_report.query('decile == 20')['cumulative_bad_rate'].reset_index(drop = True)[0]\n",
    "  summary.index = [['roc_auc','gini','ks','capture_rate_10%','capture_rate_20%']]\n",
    "  summary = summary.reset_index().rename(columns = {'level_0': 'metric'})\n",
    "  return train_report, val_report, summary\n",
    "\n",
    "def get_temporary_directory_path(prefix, suffix):\n",
    "  temp = tempfile.NamedTemporaryFile(prefix = prefix, suffix = suffix)\n",
    "  return temp\n",
    "\n",
    "def save_artifact(artifact, file_name, file_type, directory):\n",
    "  temp_file_name = get_temporary_directory_path(file_name, file_type)\n",
    "  temp_name = temp_file_name.name\n",
    "  artifact.to_csv(temp_name, index = False)\n",
    "  mlflow.log_artifact(temp_name, directory)\n",
    "\n",
    "def mlflow_run(run_name, params, n, x_train, y_train, w_train, x_val, y_val, w_val):\n",
    "  with mlflow.start_run(run_name = run_name) as run:\n",
    "    run_id = run.info.run_uuid\n",
    "    experiment_id = run.info.experiment_id\n",
    "    \n",
    "    model = xgb.XGBClassifier()\n",
    "    model.set_params(**params)\n",
    "    model.fit(x_train, y_train, sample_weight = w_train, eval_set = [(x_val, y_val, w_val)], eval_metric = 'auc', early_stopping_rounds = 8)\n",
    "\n",
    "    y_hat_train = model.predict_proba(x_train , ntree_limit = model.best_ntree_limit)[:,1]\n",
    "    y_hat_val = model.predict_proba(x_val, ntree_limit = model.best_ntree_limit)[:,1]\n",
    "    \n",
    "    feature_importance = calculate_feature_importance(model, x_train)\n",
    "    \n",
    "    train_event_rate = y_train.mean()\n",
    "    train_predicted_event_rate = y_hat_train.mean()\n",
    "    val_event_rate = y_val.mean()\n",
    "    val_predicted_event_rate = y_hat_val.mean()\n",
    "    \n",
    "    train_min_prob = y_hat_train.min()\n",
    "    train_max_prob = y_hat_train.max()\n",
    "    val_min_prob = y_hat_val.min()\n",
    "    val_max_prob = y_hat_val.max()\n",
    "    \n",
    "    best_n_tree  = model.best_ntree_limit\n",
    "    \n",
    "    train_report, val_report, summary = model_evaluation_report(y_train, y_hat_train, y_val, y_hat_val, w_train, w_val, bins = 10, mode = None)\n",
    "    \n",
    "    train_roc_auc_scr = summary['train'][0]\n",
    "    val_roc_auc_scr = summary['validation'][0]\n",
    "    train_val_roc_auc_scr_diff = train_roc_auc_scr - val_roc_auc_scr\n",
    "    \n",
    "    train_gini = summary['train'][1]\n",
    "    val_gini = summary['validation'][1]\n",
    "    train_val_gini_diff = train_gini - val_gini\n",
    "    \n",
    "    train_ks = summary['train'][2]\n",
    "    val_ks = summary['validation'][2]\n",
    "    train_val_ks_diff = train_ks - val_ks\n",
    "    \n",
    "    train_capture_rate_10 = summary['train'][3]\n",
    "    val_capture_rate_10 = summary['validation'][3]\n",
    "    train_val_capture_rate_10_diff = train_capture_rate_10 - val_capture_rate_10\n",
    "    \n",
    "    train_capture_rate_20 = summary['train'][4]\n",
    "    val_capture_rate_20 = summary['validation'][4]\n",
    "    train_val_capture_rate_20_diff = train_capture_rate_20 - val_capture_rate_20\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param(\"iteration\", n + 1)\n",
    "    \n",
    "    mlflow.log_metric(\"roc_auc_score_train\", train_roc_auc_scr)\n",
    "    mlflow.log_metric(\"roc_auc_score_validation\", val_roc_auc_scr)\n",
    "    mlflow.log_metric(\"roc_auc_score_difference\", train_val_roc_auc_scr_diff)\n",
    "    \n",
    "    mlflow.log_metric(\"gini_train\", train_gini)\n",
    "    mlflow.log_metric(\"gini_validation\", val_gini)\n",
    "    mlflow.log_metric(\"gini_difference\", train_val_gini_diff)\n",
    "    \n",
    "    mlflow.log_metric(\"ks_train\", train_ks)\n",
    "    mlflow.log_metric(\"ks_validation\", val_ks)\n",
    "    mlflow.log_metric(\"ks_difference\", train_val_ks_diff)\n",
    "    \n",
    "    mlflow.log_metric(\"capture_rate_10_train\", train_capture_rate_10)\n",
    "    mlflow.log_metric(\"capture_rate_10_validation\", val_capture_rate_10)\n",
    "    mlflow.log_metric(\"capture_rate_10_difference\", train_val_capture_rate_10_diff)\n",
    "    \n",
    "    mlflow.log_metric(\"capture_rate_20_train\", train_capture_rate_20)\n",
    "    mlflow.log_metric(\"capture_rate_20_validation\", val_capture_rate_20)\n",
    "    mlflow.log_metric(\"capture_rate_20_difference\", train_val_capture_rate_20_diff)\n",
    "    \n",
    "    mlflow.log_metric(\"actual_bad_rate_train\", train_event_rate)\n",
    "    mlflow.log_metric(\"predicted_bad_rate_train\", train_predicted_event_rate)\n",
    "    mlflow.log_metric(\"actual_bad_rate_validation\", val_event_rate)\n",
    "    mlflow.log_metric(\"predicted_bad_rate_validation\", val_predicted_event_rate)\n",
    "    \n",
    "    mlflow.log_metric(\"minimum_probability_train\", train_min_prob)\n",
    "    mlflow.log_metric(\"maximum_probability_train\", train_max_prob)\n",
    "    mlflow.log_metric(\"minimum_probability_validation\", val_min_prob)\n",
    "    mlflow.log_metric(\"maximum_probability_validation\", val_max_prob)\n",
    "\n",
    "    MlflowClient().set_experiment_tag(experiment_id, \"mlflow.note.content\", \"US Export Propensity Model Development\")\n",
    "    MlflowClient().set_tag(run_id, \"mlflow.note.content\", \"Iteration Number: {}\".format(n + 1))\n",
    "\n",
    "    mlflow.xgboost.log_model(xgb_model = model, artifact_path = \"xgboost-model\")\n",
    "    \n",
    "    save_artifact(feature_importance, \"feature_importance\", \".csv\", \"xgboost-model\")\n",
    "    save_artifact(train_report, \"model_evaluation_report_train\", \".csv\", \"xgboost-model\")\n",
    "    save_artifact(val_report, \"model_evaluation_report_validation\", \".csv\", \"xgboost-model\")\n",
    "    save_artifact(summary, \"model_evaluation_summary\", \".csv\", \"xgboost-model\")\n",
    "    \n",
    "  return (run_id, experiment_id)\n",
    "      \n",
    "\n",
    "def build_models(run_name, parameters, niter, x_train, y_train, w_train, x_val, y_val, w_val):\n",
    "\n",
    "  for n in range(niter):\n",
    "    params = {}\n",
    "    params_tmp = eval(parameters)\n",
    "\n",
    "    for i in ['colsample_bylevel', 'colsample_bytree', 'learning_rate', 'max_depth', 'min_child_weight', 'n_estimators', 'subsample']:\n",
    "      params[i] = params_tmp[i]\n",
    "\n",
    "    run_id, experiment_id = mlflow_run(run_name, params, n, x_train, y_train, w_train, x_val, y_val, w_val)\n",
    "  \n",
    "  return run_id, experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdea2b8d-4936-4603-965f-e8cdc51d7d05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/dbfs/FileStore/sahayk/us_export_propensity/output_data/us_export_propensity_model_output_override.csv')\n",
    "\n",
    "identifiers = ['duns',\n",
    "'append_year',\n",
    "'append_month',\n",
    "'sample_type']\n",
    "\n",
    "weight = 'weight'\n",
    "\n",
    "target = 'export'\n",
    "\n",
    "prediction = 'predicted_export'\n",
    "\n",
    "predictors = ['gctrs_ttl_signal_3yrs',\n",
    "'ind_gctrs_3yrs',\n",
    "'gctrs_cnt_unq_yrs',\n",
    "'npayexp',\n",
    "'sic4_score',\n",
    "'nloc',\n",
    "'gctrs_cnt_unq_customer_country',\n",
    "'nrectyp',\n",
    "'satis',\n",
    "'sales',\n",
    "'drp_paydex1_loc_decile',\n",
    "'location_growth_score',\n",
    "'ncomptype',\n",
    "'nimptexpt',\n",
    "'foreign_trade_buyer_ind',\n",
    "'location_cluster_score',\n",
    "'miny_ownd_ind',\n",
    "'export_job_title_ind',\n",
    "'ba_sum_excl_12m',\n",
    "'loc_pct_rent_1',\n",
    "'sml_bus_ind',\n",
    "'loc_pct_comptype_g',\n",
    "'export_business_name_ind',\n",
    "'ucc_flng_3yr_cnt',\n",
    "'ba_count_info_src_12m',\n",
    "'chg_tot_emp',\n",
    "'inds_norm_pydx_scr',\n",
    "'drp_sales_loc_decile',\n",
    "'foreign_trade_ind']\n",
    "\n",
    "df2 = df1.drop(prediction, axis = 1)\n",
    "\n",
    "train = df2[df2['sample_type'] == 'train']\n",
    "val = df2[df2['sample_type'] == 'val']\n",
    "test = df2[df2['sample_type'] == 'test']\n",
    "\n",
    "run_name = \"US Export Propensity Hyperparamter Tuning Experiment\"\n",
    "\n",
    "parameters = \"\"\"{'colsample_bylevel': np.random.randint(5,10)/10,\n",
    "               'colsample_bytree': np.random.randint(5,10)/10,\n",
    "               'learning_rate': np.random.choice([0.1, 0.01, 0.05, 0.075]),\n",
    "               'max_depth': np.random.randint(3,6),\n",
    "               'min_child_weight': np.random.randint(3,10),\n",
    "               'n_estimators': np.random.choice([300, 400, 500, 600]),\n",
    "               'subsample': np.random.randint(5,10)/10}\"\"\"\n",
    "\n",
    "niter = 20\n",
    "\n",
    "run_id, experiment_id = build_models(run_name, parameters, niter, train[predictors], train[target], train[weight], val[predictors], val[target], val[weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39a61f2a-0608-4f03-b678-719708eba0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model_run_id = 'cc75d84cd2bd4f16a4f8bf3d4e86a0ea'\n",
    "model_name = 'us_export_propensity'\n",
    "model_source = 'dbfs:/databricks/mlflow-tracking/1890214478686766/cc75d84cd2bd4f16a4f8bf3d4e86a0ea/artifacts/xgboost-model'\n",
    "model_version = 1\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.create_registered_model(model_name)\n",
    "model_version = client.create_model_version(model_name, model_source, best_model_run_id)\n",
    "\n",
    "client.transition_model_version_stage(name = model_name, version = model_version, stage = \"Staging\")\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri = f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "test['predicted_export'] = model.predict(test[predictors])\n",
    "\n",
    "client.transition_model_version_stage(name = model_name, version = model_version, stage = \"Production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a73110a-73fa-41cc-843b-622a1d30d236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01",
   "notebookOrigID": 1890214478686766,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
